#!/usr/bin/env python3
"""
ì‚¬ì „ ìƒì„±ëœ ì„ë² ë”©ì„ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. ì‚¬ì „ ìƒì„±ëœ ì„ë² ë”© CSV/Pickle íŒŒì¼ë“¤ì„ ë¡œë“œ
2. ì„ë² ë”©ì„ ê²°í•©í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì„±
3. í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì ìš© (K-means, HDBSCAN)
4. ì‹œê°í™” ë° ê²°ê³¼ ì €ì¥
"""

import pandas as pd
import numpy as np
import pickle
import os
import glob
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Tuple
from tqdm import tqdm
import warnings
import ast
warnings.filterwarnings('ignore')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score
    import hdbscan
    import umap.umap_ as umap
    UMAP = umap.UMAP
    import matplotlib.pyplot as plt
    import seaborn as sns
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
except ImportError as e:
    print(f"í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("pip install scikit-learn hdbscan umap-learn matplotlib seaborn plotly")
    exit(1)

class EmbeddingClusteringSystem:
    """ì‚¬ì „ ìƒì„±ëœ ì„ë² ë”©ì„ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.embeddings_df = None
        self.embeddings = None
        self.cluster_labels = None
        self.reduced_embeddings = None
        self.results = {}
        
        print(f"ğŸš€ Embedding Clustering System ì´ˆê¸°í™”")
        
    def load_embedding_files(self, input_dir: str, file_format: str = 'csv') -> Optional[pd.DataFrame]:
        """
        ì„ë² ë”© íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê²°í•©
        
        Args:
            input_dir: ì„ë² ë”© íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            file_format: íŒŒì¼ í˜•ì‹ ('csv' ë˜ëŠ” 'pickle')
            
        Returns:
            ê²°í•©ëœ ë°ì´í„°í”„ë ˆì„ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        print(f"ğŸ“‚ ì„ë² ë”© íŒŒì¼ ë¡œë”© ì¤‘: {input_dir}")
        print(f"ğŸ“„ íŒŒì¼ í˜•ì‹: {file_format.upper()}")
        
        # íŒŒì¼ íŒ¨í„´ ì„¤ì •
        if file_format.lower() == 'csv':
            file_pattern = os.path.join(input_dir, "embeddings_batch_*.csv")
        elif file_format.lower() == 'pickle':
            file_pattern = os.path.join(input_dir, "embeddings_batch_*.pkl")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_format}")
            return None
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        embedding_files = sorted(glob.glob(file_pattern))
        
        if not embedding_files:
            print(f"âŒ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_pattern}")
            return None
        
        print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(embedding_files)}ê°œ")
        
        all_dataframes = []
        
        # ê° íŒŒì¼ ë¡œë“œ
        for i, file_path in enumerate(tqdm(embedding_files, desc="íŒŒì¼ ë¡œë”©")):
            try:
                if file_format.lower() == 'csv':
                    # CSV íŒŒì¼ ë¡œë“œ
                    df = pd.read_csv(file_path)
                    
                    # ì„ë² ë”© ì»¬ëŸ¼ì´ ë¬¸ìì—´ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    if 'qwen3_embedding' in df.columns:
                        df['qwen3_embedding'] = df['qwen3_embedding'].apply(ast.literal_eval)
                    
                elif file_format.lower() == 'pickle':
                    # Pickle íŒŒì¼ ë¡œë“œ
                    with open(file_path, 'rb') as f:
                        df = pickle.load(f)
                
                all_dataframes.append(df)
                print(f"   âœ… íŒŒì¼ {i+1}/{len(embedding_files)}: {len(df)}ê°œ í–‰ ë¡œë“œ")
                
            except Exception as e:
                print(f"   âŒ íŒŒì¼ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
                continue
        
        if not all_dataframes:
            print("âŒ ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ê²°í•©
        combined_df = pd.concat(all_dataframes, ignore_index=True)
        
        print(f"âœ… ì„ë² ë”© ë¡œë”© ì™„ë£Œ")
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜: {len(combined_df):,}ê°œ")
        print(f"ğŸ“‹ ì»¬ëŸ¼: {list(combined_df.columns)}")
        
        # ì„ë² ë”© ìœ íš¨ì„± ê²€ì‚¬
        if 'qwen3_embedding' not in combined_df.columns:
            print("âŒ 'qwen3_embedding' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì„ë² ë”© ì°¨ì› í™•ì¸
        first_embedding = combined_df['qwen3_embedding'].iloc[0]
        embedding_dim = len(first_embedding) if isinstance(first_embedding, list) else 0
        print(f"ğŸ”¢ ì„ë² ë”© ì°¨ì›: {embedding_dim}")
        
        self.embeddings_df = combined_df
        
        return combined_df
    
    def extract_embeddings(self) -> Optional[np.ndarray]:
        """
        ë°ì´í„°í”„ë ˆì„ì—ì„œ ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ
        
        Returns:
            ì„ë² ë”© ë°°ì—´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        if self.embeddings_df is None:
            print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
        
        print("ğŸ”„ ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ ì¤‘...")
        
        try:
            # ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_list = self.embeddings_df['qwen3_embedding'].tolist()
            embeddings_array = np.array(embeddings_list, dtype=np.float32)
            
            print(f"âœ… ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ ì™„ë£Œ: {embeddings_array.shape}")
            
            self.embeddings = embeddings_array
            
            return embeddings_array
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def perform_clustering(self, algorithm: str = 'kmeans', **kwargs) -> Optional[np.ndarray]:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            algorithm: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ('kmeans', 'hdbscan')
            **kwargs: ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„°
            
        Returns:
            í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.")
            return None
        
        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {algorithm.upper()}")
        print(f"ğŸ“Š ì„ë² ë”© í˜•íƒœ: {self.embeddings.shape}")
        
        try:
            if algorithm.lower() == 'kmeans':
                n_clusters = kwargs.get('n_clusters', 8)
                clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
                cluster_labels = clusterer.fit_predict(self.embeddings)
                
                print(f"ğŸ“ˆ K-means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (k={n_clusters})")
                
            elif algorithm.lower() == 'hdbscan':
                min_cluster_size = kwargs.get('min_cluster_size', 15)
                min_samples = kwargs.get('min_samples', 5)
                
                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples,
                    metric='euclidean',
                    cluster_selection_method='eom'
                )
                cluster_labels = clusterer.fit_predict(self.embeddings)
                
                print(f"ğŸ“ˆ HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (min_size={min_cluster_size})")
                
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜: {algorithm}")
                return None
            
            # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
            print(f"   - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise}")
            print(f"   - ì „ì²´ í¬ì¸íŠ¸: {len(cluster_labels)}")
            
            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚° (ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œì™¸)
            if n_clusters > 1:
                mask = cluster_labels != -1
                if np.sum(mask) > 1:
                    silhouette_avg = silhouette_score(self.embeddings[mask], cluster_labels[mask])
                    print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {silhouette_avg:.3f}")
            
            self.cluster_labels = cluster_labels
            
            return cluster_labels
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_clusters(self) -> dict:
        """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¶„ì„"""
        if self.cluster_labels is None or self.embeddings_df is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print("ğŸ“ˆ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
        
        analysis = {}
        unique_labels = set(self.cluster_labels)
        
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ í´ëŸ¬ìŠ¤í„°
                continue
            
            cluster_mask = self.cluster_labels == label
            cluster_data = self.embeddings_df[cluster_mask]
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ˜í”Œ ë¬¸ì„œ
            sample_titles = cluster_data['title'].head(5).tolist() if 'title' in cluster_data.columns else []
            
            analysis[label] = {
                'size': int(np.sum(cluster_mask)),
                'sample_titles': sample_titles
            }
        
        self.results['cluster_analysis'] = analysis
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ë° ë¹„ìœ¨ ì¶œë ¥
        total_docs = len(self.cluster_labels)
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ë¶„í¬:")
        for label, info in analysis.items():
            percentage = (info['size'] / total_docs) * 100
            print(f"   í´ëŸ¬ìŠ¤í„° {label}: {info['size']}ê°œ ë¬¸ì„œ ({percentage:.1f}%)")
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë„ ì¶œë ¥
        noise_count = list(self.cluster_labels).count(-1)
        if noise_count > 0:
            noise_percentage = (noise_count / total_docs) * 100
            print(f"   ë…¸ì´ì¦ˆ: {noise_count}ê°œ ë¬¸ì„œ ({noise_percentage:.1f}%)")
        
        return analysis
    
    def reduce_dimensions(self, n_components: int = 2) -> Optional[np.ndarray]:
        """ì°¨ì› ì¶•ì†Œ (ì‹œê°í™”ìš©)"""
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ğŸ”„ ì°¨ì› ì¶•ì†Œ ì¤‘... ({n_components}ì°¨ì›)")
        
        try:
            reducer = UMAP(
                n_components=n_components,
                n_neighbors=15,
                min_dist=0.1,
                metric='cosine',
                random_state=42
            )
            
            reduced_embeddings = reducer.fit_transform(self.embeddings)
            reduced_embeddings = np.asarray(reduced_embeddings, dtype=np.float32)
            
            self.reduced_embeddings = reduced_embeddings
            
            print(f"âœ… ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {reduced_embeddings.shape}")
            return reduced_embeddings
            
        except Exception as e:
            print(f"âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {e}")
            return None
    
    def visualize_clusters(self, save_path: Optional[str] = None):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”"""
        if self.reduced_embeddings is None:
            print("ğŸ”„ ì°¨ì› ì¶•ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤...")
            if self.reduce_dimensions() is None:
                print("âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨ë¡œ ì‹œê°í™”ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return
        
        if self.cluster_labels is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ¨ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        try:
            # Matplotlib ì‹œê°í™”
            plt.figure(figsize=(12, 8))
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ ì§€ì •
            unique_labels = set(self.cluster_labels)
            colors = plt.cm.get_cmap('Set3')(np.linspace(0, 1, len(unique_labels)))
            
            for label, color in zip(unique_labels, colors):
                if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
                    color = 'black'
                    marker = 'x'
                    alpha = 0.5
                else:
                    marker = 'o'
                    alpha = 0.7
                
                mask = self.cluster_labels == label
                plt.scatter(
                    self.reduced_embeddings[mask, 0],
                    self.reduced_embeddings[mask, 1],
                    c=[color],
                    marker=marker,
                    alpha=alpha,
                    s=50,
                    label=f'Cluster {label}' if label != -1 else 'Noise'
                )
            
            plt.title('Document Clustering Results (UMAP Dimensionality Reduction)', fontsize=16)
            plt.xlabel('UMAP Component 1', fontsize=12)
            plt.ylabel('UMAP Component 2', fontsize=12)
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ğŸ’¾ ì‹œê°í™” ì €ì¥: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def save_clustering_results(self, output_path: str = "./clustering_results.csv"):
        """
        í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€í•˜ì—¬ ì €ì¥
        
        Args:
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if self.embeddings_df is None or self.cluster_labels is None:
            print("âŒ ë°ì´í„° ë˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        result_df = self.embeddings_df.copy()
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
        result_df['cluster'] = self.cluster_labels
        
        # ì„ë² ë”© ì»¬ëŸ¼ ì œê±° (ìš©ëŸ‰ ì ˆì•½)
        if 'qwen3_embedding' in result_df.columns:
            result_df = result_df.drop('qwen3_embedding', axis=1)
        
        # í´ëŸ¬ìŠ¤í„°ë§ í†µê³„ ì •ë³´ ì¶œë ¥ (ë¹„ìœ¨ í¬í•¨)
        cluster_counts = pd.Series(self.cluster_labels).value_counts().sort_index()
        total_docs = len(self.cluster_labels)
        
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ë¶„í¬:")
        for cluster_id, count in cluster_counts.items():
            percentage = (count / total_docs) * 100
            if cluster_id == -1:
                print(f"   ë…¸ì´ì¦ˆ: {count}ê°œ ({percentage:.1f}%)")
            else:
                print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: {count}ê°œ ({percentage:.1f}%)")
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“‹ ì €ì¥ëœ ì»¬ëŸ¼: {list(result_df.columns)}")
        
        return result_df
    
    def save_results(self, output_dir: str = "./clustering_results"):
        """ì „ì²´ ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
        if self.cluster_labels is not None:
            csv_path = output_path / f"embedding_clustering_results_{timestamp}.csv"
            self.save_clustering_results(str(csv_path))
        
        # ì‹œê°í™” ì €ì¥
        if self.reduced_embeddings is not None:
            viz_path = output_path / f"embedding_cluster_visualization_{timestamp}.png"
            self.visualize_clusters(save_path=str(viz_path))

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì‚¬ì „ ìƒì„±ëœ ì„ë² ë”©ì„ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§')
    parser.add_argument('--input-dir', default='./embeddings_output', help='ì„ë² ë”© íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--file-format', default='csv', choices=['csv', 'pickle'], 
                       help='ì„ë² ë”© íŒŒì¼ í˜•ì‹')
    parser.add_argument('--algorithm', default='kmeans', choices=['kmeans', 'hdbscan'], 
                       help='í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜')
    parser.add_argument('--n-clusters', type=int, default=6, help='í´ëŸ¬ìŠ¤í„° ìˆ˜ (K-meansìš©)')
    parser.add_argument('--min-cluster-size', type=int, default=6, help='ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (HDBSCANìš©)')
    parser.add_argument('--output-dir', default='./clustering_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-file', default='./clustering_results/clustering_results.csv', help='í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('--no-visualization', action='store_true', help='ì‹œê°í™” ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    print("ğŸš€ ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘!")
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    clustering_system = EmbeddingClusteringSystem()
    
    # 1. ì„ë² ë”© íŒŒì¼ ë¡œë“œ
    embeddings_df = clustering_system.load_embedding_files(args.input_dir, args.file_format)
    
    if embeddings_df is None:
        print("âŒ ì„ë² ë”© íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # 2. ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ
    embeddings = clustering_system.extract_embeddings()
    
    if embeddings is None:
        print("âŒ ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ ì‹¤íŒ¨")
        return
    
    # 3. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    if args.algorithm == 'kmeans':
        cluster_labels = clustering_system.perform_clustering('kmeans', n_clusters=args.n_clusters)
    else:
        cluster_labels = clustering_system.perform_clustering('hdbscan', min_cluster_size=args.min_cluster_size)
    
    if cluster_labels is None:
        print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
        return
    
    # 4. ê²°ê³¼ ë¶„ì„
    clustering_system.analyze_clusters()
    
    # 5. ì‹œê°í™”
    if not args.no_visualization:
        clustering_system.visualize_clusters()
    
    # 6. ê²°ê³¼ ì €ì¥
    clustering_system.save_results(args.output_dir)
    clustering_system.save_clustering_results(args.output_file)
    
    print("ğŸ‰ ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 