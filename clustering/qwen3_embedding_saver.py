#!/usr/bin/env python3
"""
Qwen3-Embedding ì„ë² ë”© ìƒì„± ë° ë°°ì¹˜ ì €ì¥ ì‹œìŠ¤í…œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. train.csv ë°ì´í„°ë¥¼ ë¡œë“œ
2. Qwen3-Embeddingìœ¼ë¡œ ì„ë² ë”© ìƒì„± (transformers ì§ì ‘ ì‚¬ìš©)
3. ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ë¥¼ ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥
4. ì›ë³¸ ë°ì´í„° + ì„ë² ë”©ê°’ì„ pickle/csvë¡œ ì €ì¥
"""

import pandas as pd
import numpy as np
import pickle
import os
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Optional
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import torch
    import torch.nn.functional as F
    from torch import Tensor
    from transformers import AutoTokenizer, AutoModel
except ImportError as e:
    print(f"í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("pip install transformers>=4.51.0 torch")
    exit(1)

def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """Last token pooling for Qwen3-Embedding"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

def get_detailed_instruct(task_description: str, query: str) -> str:
    """Get detailed instruction for Qwen3-Embedding"""
    return f'Instruct: {task_description}\nQuery: {query}'

class EmbeddingSaver:
    """Qwen3-Embedding ì„ë² ë”© ìƒì„± ë° ë°°ì¹˜ ì €ì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.max_length = 4096
        self.data_df = None
        
        # ì£¼ì œ ë¶„ë¥˜ë¥¼ ìœ„í•œ task description (ì˜ì–´)
        self.task_description = "Please analyze the topic and content of each document based on its title and content to enable future grouping of similar documents by subject."
        
        print(f"ğŸš€ Embedding Saver ì´ˆê¸°í™”")
        print(f"ğŸ“Š ì‚¬ìš© ëª¨ë¸: {model_name}")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (Qwen3-Embedding ì§ì ‘ ì‚¬ìš©)"""
        try:
            print(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                padding_side='left'
            )
            
            # ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ ì„¤ì •)
            self.model = AutoModel.from_pretrained(self.model_name).to(self.device)
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.tokenizer = None
            self.model = None
            
    def load_data(self, csv_path: str, sample_size: Optional[int] = None) -> Optional[pd.DataFrame]:
        """
        ë°ì´í„° ë¡œë“œ
        
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            sample_size: ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
            
        Returns:
            ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„
        """
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['title', 'full_text', 'generated']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            print(f"ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            return None
        
        # ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42)
            print(f"ğŸ¯ ìƒ˜í”Œë§ ì™„ë£Œ: {len(df)}ê°œ ë°ì´í„°")
        
        self.data_df = df
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
        
        return df
    
    def prepare_text(self, title: str, full_text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì¤€ë¹„ (title + full_text ê²°í•©)
        
        Args:
            title: ì œëª©
            full_text: ë³¸ë¬¸
            
        Returns:
            ê²°í•©ëœ í…ìŠ¤íŠ¸
        """
        title = str(title) if pd.notna(title) else ""
        full_text = str(full_text) if pd.notna(full_text) else ""
        
        # titleê³¼ full_textë¥¼ ê°„ë‹¨í•˜ê²Œ ê²°í•©
        if title and full_text:
            combined_text = f"This is the title: {title}\n\nThis is the full text: {full_text}"
        elif title:
            combined_text = title
        elif full_text:
            combined_text = full_text
        else:
            combined_text = ""
        
        return combined_text
    
    def generate_embeddings_batch(self, texts: List[str], batch_size: int = 4) -> np.ndarray:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± (Qwen3-Embedding ì§ì ‘ ì‚¬ìš©)
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì„ë² ë”© ë°°ì—´
        """
        if self.tokenizer is None or self.model is None:
            self.load_embedding_model()
        
        if self.tokenizer is None or self.model is None:
            print("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            return np.array([])
        
        print(f"ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # ì£¼ì œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
        query_texts = []
        for text in texts:
            query_text = get_detailed_instruct(self.task_description, text)
            query_texts.append(query_text)
        
        all_embeddings = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in tqdm(range(0, len(query_texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
            batch_texts = query_texts[i:i + batch_size]
            
            try:
                # í† í¬ë‚˜ì´ì§•
                batch_dict = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt",
                )
                batch_dict = {k: v.to(self.device) for k, v in batch_dict.items()}
                
                # ëª¨ë¸ ì¶”ë¡ 
                with torch.no_grad():
                    outputs = self.model(**batch_dict)
                    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
                    
                    # ì„ë² ë”© ì •ê·œí™”
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                    
                    # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                    embeddings = embeddings.cpu().numpy()
                    all_embeddings.append(embeddings)
                    
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                # ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
                empty_embeddings = np.zeros((len(batch_texts), 1024), dtype=np.float32)
                all_embeddings.append(empty_embeddings)
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        if all_embeddings:
            final_embeddings = np.vstack(all_embeddings)
        else:
            final_embeddings = np.array([])
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {final_embeddings.shape}")
        
        return final_embeddings
    
    def save_batch_data(self, batch_df: pd.DataFrame, batch_embeddings: np.ndarray, 
                       output_dir: str, batch_idx: int, save_format: str = 'pickle'):
        """
        ë°°ì¹˜ ë°ì´í„° ì €ì¥
        
        Args:
            batch_df: ë°°ì¹˜ ë°ì´í„°í”„ë ˆì„
            batch_embeddings: ë°°ì¹˜ ì„ë² ë”©
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
            save_format: ì €ì¥ í˜•ì‹ ('pickle' ë˜ëŠ” 'csv')
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë°°ì¹˜ ë°ì´í„°í”„ë ˆì„ì— ì„ë² ë”© ì¶”ê°€
        result_df = batch_df.copy()
        result_df['qwen3_embedding'] = [emb.tolist() for emb in batch_embeddings]
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if save_format.lower() == 'pickle':
            # Pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥
            filename = f"embeddings_batch_{batch_idx:04d}_{timestamp}.pkl"
            filepath = os.path.join(output_dir, filename)
            
            with open(filepath, 'wb') as f:
                pickle.dump(result_df, f)
            
            print(f"ğŸ’¾ ë°°ì¹˜ {batch_idx} ì €ì¥ ì™„ë£Œ (Pickle): {filepath}")
            
        elif save_format.lower() == 'csv':
            # CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì„ë² ë”©ì€ ë¬¸ìì—´ë¡œ ë³€í™˜)
            filename = f"embeddings_batch_{batch_idx:04d}_{timestamp}.csv"
            filepath = os.path.join(output_dir, filename)
            
            # ì„ë² ë”©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
            result_df['qwen3_embedding'] = result_df['qwen3_embedding'].apply(str)
            result_df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ’¾ ë°°ì¹˜ {batch_idx} ì €ì¥ ì™„ë£Œ (CSV): {filepath}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del result_df
        
        return filepath
    
    def process_and_save(self, csv_path: str, save_interval: int = 10000, 
                        batch_size: int = 4, output_dir: str = "./embeddings_output",
                        save_format: str = 'pickle', sample_size: Optional[int] = None):
        """
        ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰: ë°ì´í„° ë¡œë“œ â†’ ì„ë² ë”© ìƒì„± â†’ ë°°ì¹˜ ì €ì¥
        
        Args:
            csv_path: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
            save_interval: ì €ì¥ ê°„ê²© (ëª‡ ê°œ ë°ì´í„°ë§ˆë‹¤ ì €ì¥í• ì§€)
            batch_size: ì„ë² ë”© ìƒì„± ë°°ì¹˜ í¬ê¸°
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            save_format: ì €ì¥ í˜•ì‹ ('pickle' ë˜ëŠ” 'csv')
            sample_size: ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜
        """
        print("="*80)
        print("ğŸš€ Qwen3-Embedding ë°°ì¹˜ ì €ì¥ ì‹œìŠ¤í…œ ì‹œì‘")
        print("="*80)
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_data(csv_path, sample_size)
        if df is None:
            print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return []
        
        total_rows = len(df)
        num_batches = (total_rows + save_interval - 1) // save_interval
        
        print(f"ğŸ“Š ì²˜ë¦¬ ê³„íš:")
        print(f"   - ì „ì²´ ë°ì´í„°: {total_rows:,}ê°œ")
        print(f"   - ì €ì¥ ê°„ê²©: {save_interval:,}ê°œ")
        print(f"   - ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {num_batches}ê°œ")
        print(f"   - ì„ë² ë”© ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   - ì €ì¥ í˜•ì‹: {save_format.upper()}")
        print(f"   - ì£¼ì œ ë¶„ë¥˜ íƒœìŠ¤í¬: {self.task_description}")
        
        saved_files = []
        
        # 2. ë°°ì¹˜ë³„ ì²˜ë¦¬
        for batch_idx in range(num_batches):
            start_idx = batch_idx * save_interval
            end_idx = min((batch_idx + 1) * save_interval, total_rows)
            
            print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{num_batches} ì²˜ë¦¬ ì¤‘ ({start_idx:,}-{end_idx-1:,})")
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_df = df.iloc[start_idx:end_idx].copy()
            
            # í…ìŠ¤íŠ¸ ì¤€ë¹„
            print("ğŸ“ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘...")
            texts = []
            for _, row in batch_df.iterrows():
                combined_text = self.prepare_text(row['title'], row['full_text'])
                texts.append(combined_text)
            
            # ì„ë² ë”© ìƒì„±
            batch_embeddings = self.generate_embeddings_batch(texts, batch_size)
            
            if len(batch_embeddings) == 0:
                print(f"âŒ ë°°ì¹˜ {batch_idx + 1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                continue
            
            # ë°°ì¹˜ ì €ì¥
            filepath = self.save_batch_data(
                batch_df, batch_embeddings, output_dir, 
                batch_idx + 1, save_format
            )
            saved_files.append(filepath)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del batch_df, texts, batch_embeddings
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        print("\n" + "="*80)
        print("ğŸ‰ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*80)
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_files)}ê°œ")
        print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"ğŸ’¾ ì €ì¥ í˜•ì‹: {save_format.upper()}")
        
        return saved_files

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Qwen3-Embedding ë°°ì¹˜ ì €ì¥ ì‹œìŠ¤í…œ')
    parser.add_argument('--data', default='/home/work/INC_share/GU/SWContest/trial_taemin/data/train.csv', help='ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--save-interval', type=int, default=1000, help='ì €ì¥ ê°„ê²© (ë°ì´í„° ê°œìˆ˜)')
    parser.add_argument('--batch-size', type=int, default=8, help='ì„ë² ë”© ìƒì„± ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--output-dir', default='./embeddings_output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--save-format', default='csv', choices=['pickle', 'csv'], 
                       help='ì €ì¥ í˜•ì‹')
    parser.add_argument('--sample-size', type=int, help='ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜')
    parser.add_argument('--model-name', default='Qwen/Qwen3-Embedding-0.6B', 
                       help='ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸')
    
    args = parser.parse_args()
    
    # ì„ë² ë”© ì €ì¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    saver = EmbeddingSaver(model_name=args.model_name)
    
    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    saved_files = saver.process_and_save(
        csv_path=args.data,
        save_interval=args.save_interval,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        save_format=args.save_format,
        sample_size=args.sample_size
    )
    
    if saved_files:
        print(f"\nâœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ! ì´ {len(saved_files)}ê°œ íŒŒì¼ ìƒì„±")
    else:
        print(f"\nâŒ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 