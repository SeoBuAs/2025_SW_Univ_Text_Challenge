#!/usr/bin/env python3
"""
Qwen3-Embedding-0.6B ê¸°ë°˜ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Optional, Tuple
import argparse
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import os
import pickle

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import torch
    import torch.nn.functional as F
    from torch import Tensor
    from transformers import AutoTokenizer, AutoModel
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score
    import hdbscan
    import umap.umap_ as umap
    UMAP = umap.UMAP
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
except ImportError as e:
    print(f"í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {e}")
    print("pip install transformers>=4.51.0 torch scikit-learn hdbscan umap-learn plotly")
    exit(1)

def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """Last token pooling for Qwen3-Embedding"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

def get_detailed_instruct(task_description: str, query: str) -> str:
    """Get detailed instruction for Qwen3-Embedding"""
    return f'Instruct: {task_description}\nQuery: {query}'

class DocumentClusteringSystem:
    """Qwen3-Embedding-0.6B ê¸°ë°˜ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.max_length = 8192
        self.documents = []
        self.embeddings = None
        self.clusters = None
        self.cluster_labels = None
        self.reduced_embeddings = None
        self.results = {}
        self.original_df = None  # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ì €ì¥ìš©
        self.valid_indices = []  # ìœ íš¨í•œ ë¬¸ì„œ ì¸ë±ìŠ¤ ì €ì¥ìš©
        
        # ì£¼ì œ ë¶„ë¥˜ë¥¼ ìœ„í•œ task description (ì˜ì–´)
        self.task_description = "Analyze the topic and content of documents to classify them by subject and group similar content and topics together"
        
        print(f"ğŸš€ Document Clustering System ì´ˆê¸°í™”")
        print(f"ğŸ“Š ì‚¬ìš© ëª¨ë¸: {model_name}")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
    
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (Qwen3-Embedding ì§ì ‘ ì‚¬ìš©)"""
        try:
            print(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                padding_side='left'
            )
            
            # ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ ì„¤ì •)
            self.model = AutoModel.from_pretrained(self.model_name).to(self.device)
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.tokenizer = None
            self.model = None
            
    def load_and_preprocess_documents(self, csv_path: str, text_column: str = 'full_text', 
                                    sample_size: Optional[int] = None) -> List[str]:
        """
        ë¬¸ì„œ ìˆ˜ì§‘ (title + full_text ê²°í•©, ì „ì²˜ë¦¬ ì—†ìŒ)
        
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            text_column: í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª… (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, title+full_text ê²°í•©)
            sample_size: ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
            
        Returns:
            ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (title + full_text ê²°í•©, ì „ì²˜ë¦¬ ì—†ìŒ)
        """
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {csv_path}")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜: {len(df)}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['title', 'full_text', 'generated']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            print(f"ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            return []
        
        # ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42)
            print(f"ğŸ¯ ìƒ˜í”Œë§ ì™„ë£Œ: {len(df)}ê°œ ë¬¸ì„œ")
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ì €ì¥ (í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¶”ê°€ìš©)
        self.original_df = df.copy()
        
        # í…ìŠ¤íŠ¸ ê²°í•© (Qwen3-Embedding ìµœì í™”)
        print("ğŸ”§ title + full_text ê²°í•© ì¤‘... (ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™”)")
        documents = []
        
        for idx, row in tqdm(df.iterrows(), desc="ë¬¸ì„œ ì²˜ë¦¬", total=len(df)):
            title = str(row['title']) if pd.notna(row['title']) else ""
            full_text = str(row['full_text']) if pd.notna(row['full_text']) else ""
            
            # titleê³¼ full_textë¥¼ ê°„ë‹¨í•˜ê²Œ ê²°í•© (Qwen3-Embeddingì´ ìë™ìœ¼ë¡œ ìµœì í™”)
            if title and full_text:
                combined_text = f"This is the title: {title}\n\nThis is the full text: {full_text}"
            elif title:
                combined_text = title
            elif full_text:
                combined_text = full_text
            else:
                combined_text = ""
            
            documents.append(combined_text)
        
        self.documents = documents
        valid_count = len([d for d in documents if d.strip()])
        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {valid_count}ê°œ ìœ íš¨ ë¬¸ì„œ / {len(documents)}ê°œ ì „ì²´")
        
        return documents
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ (í•œêµ­ì–´ íŠ¹í™”)"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # URL ì œê±°
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ì´ë©”ì¼ ì œê±°
        text = re.sub(r'\S+@\S+', '', text)
        
        # í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± í†µì¼
        text = re.sub(r'\s+', ' ', text)
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±° (1ê¸€ì ë‹¨ì–´)
        words = text.split()
        words = [word for word in words if len(word) > 1]
        text = ' '.join(words)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def generate_embeddings(self, batch_size: int = 4) -> Optional[np.ndarray]:
        """
        ì„ë² ë”© ìƒì„± (Qwen3-Embedding ì§ì ‘ ì‚¬ìš©)
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì •ê·œí™”ëœ ì„ë² ë”© ë°°ì—´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        if not self.documents:
            print("âŒ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
            
        if self.tokenizer is None or self.model is None:
            self.load_embedding_model()
            
        # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì²´í¬
        if self.tokenizer is None or self.model is None:
            print("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        print(f"ğŸš€ ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œ ìµœì í™” ëª¨ë“œ (512GB RAM, 48GB VRAM)")
        
        try:
            # ìœ íš¨í•œ ë¬¸ì„œë§Œ ì„ë² ë”© ìƒì„±
            valid_documents = [doc for doc in self.documents if doc.strip()]
            valid_indices = [i for i, doc in enumerate(self.documents) if doc.strip()]
            
            print(f"ğŸ“Š ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(valid_documents)} / {len(self.documents)}")
            
            if not valid_documents:
                print("âŒ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì£¼ì œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
            query_texts = []
            for doc in valid_documents:
                query_text = get_detailed_instruct(self.task_description, doc)
                query_texts.append(query_text)
            
            all_embeddings = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in tqdm(range(0, len(query_texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
                batch_texts = query_texts[i:i + batch_size]
                
                try:
                    # í† í¬ë‚˜ì´ì§•
                    batch_dict = self.tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=self.max_length,
                        return_tensors="pt",
                    )
                    batch_dict = {k: v.to(self.device) for k, v in batch_dict.items()}
                    
                    # ëª¨ë¸ ì¶”ë¡ 
                    with torch.no_grad():
                        outputs = self.model(**batch_dict)
                        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])
                        
                        # ì„ë² ë”© ì •ê·œí™”
                        embeddings = F.normalize(embeddings, p=2, dim=1)
                        
                        # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                        embeddings = embeddings.cpu().numpy()
                        all_embeddings.append(embeddings)
                        
                except Exception as e:
                    print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    # ë¹ˆ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
                    empty_embeddings = np.zeros((len(batch_texts), 1024), dtype=np.float32)
                    all_embeddings.append(empty_embeddings)
            
            # ëª¨ë“  ë°°ì¹˜ ê²°í•©
            if all_embeddings:
                valid_embeddings = np.vstack(all_embeddings)
            else:
                print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return None
            
            # ì „ì²´ ë¬¸ì„œ ìˆ˜ì— ë§ê²Œ ì„ë² ë”© ë°°ì—´ ìƒì„± (ë¹ˆ ë¬¸ì„œëŠ” 0ìœ¼ë¡œ ì±„ì›€)
            embedding_dim = valid_embeddings.shape[1]
            all_embeddings = np.zeros((len(self.documents), embedding_dim), dtype=np.float32)
            
            # ìœ íš¨í•œ ë¬¸ì„œì˜ ì„ë² ë”©ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, valid_idx in enumerate(valid_indices):
                all_embeddings[valid_idx] = valid_embeddings[i]
            
            self.embeddings = all_embeddings
            self.valid_indices = valid_indices  # ìœ íš¨í•œ ì¸ë±ìŠ¤ ì €ì¥
            
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {all_embeddings.shape}")
            print(f"ğŸ“Š ìœ íš¨ ì„ë² ë”©: {len(valid_indices)}ê°œ")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def load_embeddings_from_files(self, embeddings_dir: str, file_pattern: str = "*.csv") -> Optional[np.ndarray]:
        """
        ì €ì¥ëœ ì„ë² ë”© íŒŒì¼ë“¤ì—ì„œ ì„ë² ë”© ë¡œë“œ
        
        Args:
            embeddings_dir: ì„ë² ë”© íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            file_pattern: íŒŒì¼ íŒ¨í„´ (ì˜ˆ: "*.csv", "*.pkl")
            
        Returns:
            ì„ë² ë”© ë°°ì—´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        import glob
        import ast
        
        print(f"ğŸ“¥ ì„ë² ë”© íŒŒì¼ ë¡œë”©: {embeddings_dir}")
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        file_path = os.path.join(embeddings_dir, file_pattern)
        embedding_files = sorted(glob.glob(file_path))
        
        if not embedding_files:
            print(f"âŒ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return None
        
        print(f"ğŸ“ ë°œê²¬ëœ ì„ë² ë”© íŒŒì¼ ìˆ˜: {len(embedding_files)}")
        
        all_embeddings = []
        all_documents = []
        all_data = []
        
        # ê° íŒŒì¼ì—ì„œ ì„ë² ë”© ë¡œë“œ
        for file_path in tqdm(embedding_files, desc="ì„ë² ë”© íŒŒì¼ ë¡œë”©"):
            try:
                if file_path.endswith('.csv'):
                    # CSV íŒŒì¼ ë¡œë“œ
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                    
                    # ì„ë² ë”© ì»¬ëŸ¼ í™•ì¸
                    if 'qwen3_embedding' not in df.columns:
                        print(f"âš ï¸ ì„ë² ë”© ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                        continue
                    
                    # ì„ë² ë”© ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    embeddings = []
                    for emb_str in df['qwen3_embedding']:
                        try:
                            if isinstance(emb_str, str):
                                # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
                                emb_list = ast.literal_eval(emb_str)
                                embeddings.append(np.array(emb_list, dtype=np.float32))
                            else:
                                embeddings.append(np.array(emb_str, dtype=np.float32))
                        except:
                            # íŒŒì‹± ì‹¤íŒ¨ì‹œ 0ìœ¼ë¡œ ì±„ì›€
                            embeddings.append(np.zeros(1024, dtype=np.float32))
                    
                    embeddings = np.array(embeddings)
                    
                elif file_path.endswith('.pkl'):
                    # Pickle íŒŒì¼ ë¡œë“œ
                    with open(file_path, 'rb') as f:
                        df = pickle.load(f)
                    
                    # ì„ë² ë”© ì¶”ì¶œ
                    embeddings = np.array([np.array(emb, dtype=np.float32) for emb in df['qwen3_embedding']])
                
                else:
                    print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
                    continue
                
                # ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
                if 'title' in df.columns and 'full_text' in df.columns:
                    for _, row in df.iterrows():
                        title = str(row['title']) if pd.notna(row['title']) else ""
                        full_text = str(row['full_text']) if pd.notna(row['full_text']) else ""
                        
                        if title and full_text:
                            combined_text = f"This is the title: {title}\n\nThis is the full text: {full_text}"
                        elif title:
                            combined_text = title
                        elif full_text:
                            combined_text = full_text
                        else:
                            combined_text = ""
                        
                        all_documents.append(combined_text)
                
                # ë°ì´í„° ìˆ˜ì§‘
                all_data.append(df)
                all_embeddings.append(embeddings)
                
                print(f"âœ… {file_path}: {len(embeddings)}ê°œ ì„ë² ë”© ë¡œë“œ")
                
            except Exception as e:
                print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        if not all_embeddings:
            print("âŒ ë¡œë“œëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ëª¨ë“  ì„ë² ë”© ê²°í•©
        self.embeddings = np.vstack(all_embeddings)
        self.documents = all_documents
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ê²°í•©
        if all_data:
            self.original_df = pd.concat(all_data, ignore_index=True)
        
        # ìœ íš¨í•œ ì¸ë±ìŠ¤ ì„¤ì • (ëª¨ë“  ì„ë² ë”©ì´ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •)
        self.valid_indices = list(range(len(self.embeddings)))
        
        print(f"âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ:")
        print(f"   - ì´ ì„ë² ë”© ìˆ˜: {len(self.embeddings)}")
        print(f"   - ì„ë² ë”© ì°¨ì›: {self.embeddings.shape[1]}")
        print(f"   - ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
        print(f"   - ì›ë³¸ ë°ì´í„°: {len(self.original_df) if self.original_df is not None else 0}ê°œ í–‰")
        
        return self.embeddings
    
    def perform_clustering(self, algorithm: str = 'hdbscan', **kwargs) -> Optional[np.ndarray]:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ìœ íš¨í•œ ë¬¸ì„œë§Œ í´ëŸ¬ìŠ¤í„°ë§)
        
        Args:
            algorithm: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ('kmeans', 'hdbscan')
            **kwargs: ì•Œê³ ë¦¬ì¦˜ë³„ íŒŒë¼ë¯¸í„°
            
        Returns:
            í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return None
            
        if not hasattr(self, 'valid_indices'):
            print("âŒ ìœ íš¨í•œ ì¸ë±ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {algorithm.upper()}")
        
        try:
            # ìœ íš¨í•œ ë¬¸ì„œì˜ ì„ë² ë”©ë§Œ ì¶”ì¶œ
            valid_embeddings = self.embeddings[self.valid_indices]
            
            if algorithm.lower() == 'kmeans':
                n_clusters = kwargs.get('n_clusters', 8)
                clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
                valid_cluster_labels = clusterer.fit_predict(valid_embeddings)
                
            elif algorithm.lower() == 'hdbscan':
                min_cluster_size = kwargs.get('min_cluster_size', 15)
                min_samples = kwargs.get('min_samples', 5)
                
                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples,
                    metric='euclidean',
                    cluster_selection_method='eom'
                )
                valid_cluster_labels = clusterer.fit_predict(valid_embeddings)
                
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜: {algorithm}")
                return None
            
            # ì „ì²´ ë¬¸ì„œ ìˆ˜ì— ë§ê²Œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´ ìƒì„± (ë¬´íš¨í•œ ë¬¸ì„œëŠ” -1ë¡œ ì„¤ì •)
            all_cluster_labels = np.full(len(self.documents), -1, dtype=int)
            
            # ìœ íš¨í•œ ë¬¸ì„œì˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, valid_idx in enumerate(self.valid_indices):
                all_cluster_labels[valid_idx] = valid_cluster_labels[i]
            
            self.cluster_labels = all_cluster_labels
            
            # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„
            n_clusters = len(set(valid_cluster_labels)) - (1 if -1 in valid_cluster_labels else 0)
            n_noise = list(valid_cluster_labels).count(-1)
            n_invalid = len(self.documents) - len(self.valid_indices)
            
            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
            print(f"   - ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise}")
            print(f"   - ë¬´íš¨ ë¬¸ì„œ: {n_invalid}")
            print(f"   - ìœ íš¨ ë¬¸ì„œ: {len(self.valid_indices)}")
            
            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚° (ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œì™¸)
            if n_clusters > 1:
                mask = valid_cluster_labels != -1
                if np.sum(mask) > 1:
                    silhouette_avg = silhouette_score(valid_embeddings[mask], valid_cluster_labels[mask])
                    print(f"   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {silhouette_avg:.3f}")
            
            return all_cluster_labels
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_clusters(self) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë¶„ì„"""
        if self.cluster_labels is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        print("ğŸ“ˆ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
        
        analysis = {}
        unique_labels = set(self.cluster_labels)
        
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ í´ëŸ¬ìŠ¤í„°
                continue
                
            cluster_docs = [self.documents[i] for i, l in enumerate(self.cluster_labels) if l == label]
            
            analysis[label] = {
                'size': len(cluster_docs),
                'documents': cluster_docs[:5],  # ìƒ˜í”Œ ë¬¸ì„œ 5ê°œ
                'avg_length': np.mean([len(doc) for doc in cluster_docs])
            }
        
        self.results['cluster_analysis'] = analysis
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ì¶œë ¥
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìˆ˜:")
        for label, info in analysis.items():
            print(f"   í´ëŸ¬ìŠ¤í„° {label}: {info['size']}ê°œ ë¬¸ì„œ")
        
        return analysis
    
    def reduce_dimensions(self, n_components: int = 3) -> Optional[np.ndarray]:
        """ì°¨ì› ì¶•ì†Œ (3ì°¨ì› ì‹œê°í™”ìš©)"""
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        print(f"ğŸ”„ ì°¨ì› ì¶•ì†Œ ì¤‘... ({n_components}ì°¨ì›)")
        
        try:
            reducer = UMAP(
                n_components=n_components,
                n_neighbors=15,
                min_dist=0.1,
                metric='cosine',
                random_state=42
            )
            
            reduced_embeddings = reducer.fit_transform(self.embeddings)
            
            # Convert to numpy array and ensure proper dtype
            reduced_embeddings = np.asarray(reduced_embeddings, dtype=np.float32)
            
            self.reduced_embeddings = reduced_embeddings
            
            print(f"âœ… ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {reduced_embeddings.shape}")
            return reduced_embeddings
        except Exception as e:
            print(f"âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {e}")
            return None
    
    def visualize_clusters(self, save_path: Optional[str] = None):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” (3ì°¨ì›, íˆ¬ëª… ë°°ê²½)"""
        if self.reduced_embeddings is None:
            print("ğŸ”„ ì°¨ì› ì¶•ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤...")
            if self.reduce_dimensions(n_components=3) is None:
                print("âŒ ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨ë¡œ ì‹œê°í™”ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return
        
        if self.cluster_labels is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        if self.reduced_embeddings is None:
            print("âŒ ì°¨ì› ì¶•ì†Œëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ¨ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” ì¤‘... (3ì°¨ì›, íˆ¬ëª… ë°°ê²½)")
        
        try:
            # 3ì°¨ì› ì‹œê°í™”ë¥¼ ìœ„í•œ ì„¤ì •
            fig = plt.figure(figsize=(15, 10))
            
            # íˆ¬ëª… ë°°ê²½ ì„¤ì •
            fig.patch.set_facecolor('none')
            
            # 3D ì„œë¸Œí”Œë¡¯ ìƒì„±
            ax = fig.add_subplot(111, projection='3d')
            ax.set_facecolor('none')
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ ì§€ì •
            unique_labels = set(self.cluster_labels)
            colors = plt.cm.get_cmap('Set3')(np.linspace(0, 1, len(unique_labels)))
            
            for label, color in zip(unique_labels, colors):
                if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
                    color = 'black'
                    marker = 'x'
                    alpha = 0.5
                    s = 30
                else:
                    marker = 'o'
                    alpha = 0.7
                    s = 50
                
                mask = self.cluster_labels == label
                ax.scatter(
                    self.reduced_embeddings[mask, 0],
                    self.reduced_embeddings[mask, 1],
                    self.reduced_embeddings[mask, 2],
                    c=[color],
                    marker=marker,
                    alpha=alpha,
                    s=s,
                    label=f'Cluster {label}' if label != -1 else 'Noise'
                )
            
            ax.set_title('Document Clustering Results (3D UMAP)', fontsize=16, fontweight='bold')
            ax.set_xlabel('UMAP Component 1', fontsize=12, fontweight='bold')
            ax.set_ylabel('UMAP Component 2', fontsize=12, fontweight='bold')
            ax.set_zlabel('UMAP Component 3', fontsize=12, fontweight='bold')
            
            # ë²”ë¡€ ì„¤ì •
            ax.legend(bbox_to_anchor=(1.15, 1), loc='upper left', fontsize=10)
            
            # ê²©ì ì„¤ì •
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                           facecolor='none', edgecolor='none', transparent=True)
                print(f"ğŸ’¾ ì‹œê°í™” ì €ì¥: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            if save_path:
                print(f"ğŸ’¡ ì‹œê°í™” íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_path}")
    
    def save_results(self, output_dir: str = "./clustering_results"):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
        if self.cluster_labels is not None:
            results_df = pd.DataFrame({
                'document': self.documents,
                'cluster': self.cluster_labels
            })
            
            csv_path = output_path / f"clustering_results_{timestamp}.csv"
            results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥: {csv_path}")
        
        # ì‹œê°í™” ì €ì¥
        if self.reduced_embeddings is not None:
            viz_path = output_path / f"cluster_visualization_{timestamp}.png"
            self.visualize_clusters(save_path=str(viz_path))

    def save_clustering_results(self, output_path: str = "./train_clustering.csv"):
        """
        í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€í•˜ì—¬ ì €ì¥
        
        Args:
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if self.original_df is None or self.cluster_labels is None:
            print("âŒ ì›ë³¸ ë°ì´í„° ë˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        result_df = self.original_df.copy()
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
        result_df['cluster'] = self.cluster_labels
        
        # í´ëŸ¬ìŠ¤í„°ë§ í†µê³„ ì •ë³´ ì¶œë ¥
        cluster_counts = pd.Series(self.cluster_labels).value_counts().sort_index()
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ë¬¸ì„œ ìˆ˜:")
        for cluster_id, count in cluster_counts.items():
            if cluster_id == -1:
                print(f"   ë…¸ì´ì¦ˆ/ë¬´íš¨: {count}ê°œ")
            else:
                print(f"   í´ëŸ¬ìŠ¤í„° {cluster_id}: {count}ê°œ")
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“‹ ì €ì¥ëœ ì»¬ëŸ¼: {list(result_df.columns)}")
        
        return result_df

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Qwen3-Embedding ê¸°ë°˜ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ (ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œ ìµœì í™”)')
    parser.add_argument('--data', default='../data/train.csv', help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--embeddings-dir', default='./embeddings_output', help='ì„ë² ë”© íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬')
    parser.add_argument('--text-column', default='full_text', help='í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…')
    parser.add_argument('--sample-size', type=int, help='ìƒ˜í”Œë§í•  ë¬¸ì„œ ìˆ˜')
    parser.add_argument('--algorithm', default='kmeans', choices=['kmeans'], 
                       help='í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ (kmeansë¡œ ê³ ì •)')
    parser.add_argument('--n-clusters', type=int, default=5, help='í´ëŸ¬ìŠ¤í„° ìˆ˜ (5ë¡œ ê³ ì •)')
    parser.add_argument('--min-cluster-size', type=int, default=10, help='ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (HDBSCANìš©)')
    parser.add_argument('--batch-size', type=int, default=4, help='ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œìš©)')
    parser.add_argument('--output-dir', default='./clustering_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--clustering-output', default='./clustering_results/train_clustering.csv', help='í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--no-visualization', action='store_true', help='ì‹œê°í™” ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--use-saved-embeddings', action='store_true', default=True, help='ì €ì¥ëœ ì„ë² ë”© ì‚¬ìš©')
    
    args = parser.parse_args()
    
    # kmeansë¡œ ê³ ì •í•˜ê³  k=5ë¡œ ì„¤ì •
    args.algorithm = 'kmeans'
    args.n_clusters = 5
    
    print("ğŸš€ ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œ ìµœì í™” ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘!")
    print(f"ğŸ’¾ ì‹œìŠ¤í…œ ì‚¬ì–‘: 512GB RAM, 48GB VRAM")
    print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •: K-means, k=5")
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    clustering_system = DocumentClusteringSystem()
    
    if args.use_saved_embeddings:
        # ì €ì¥ëœ ì„ë² ë”© íŒŒì¼ë“¤ì—ì„œ ë¡œë“œ
        embeddings = clustering_system.load_embeddings_from_files(args.embeddings_dir)
        if embeddings is None:
            print("âŒ ì €ì¥ëœ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨")
            return
    else:
        # ê¸°ì¡´ ë°©ì‹: ë¬¸ì„œ ë¡œë“œ â†’ ì„ë² ë”© ìƒì„±
        documents = clustering_system.load_and_preprocess_documents(
            args.data, 
            args.text_column, 
            args.sample_size
        )
        
        if not documents:
            print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„ë² ë”© ìƒì„± (ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œ ìµœì í™”)
        embeddings = clustering_system.generate_embeddings(batch_size=args.batch_size)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (kmeans, k=5ë¡œ ê³ ì •)
    cluster_labels = clustering_system.perform_clustering('kmeans', n_clusters=5)
    
    # ê²°ê³¼ ë¶„ì„
    clustering_system.analyze_clusters()
    
    # ì‹œê°í™”
    if not args.no_visualization:
        clustering_system.visualize_clusters()
    
    # ê²°ê³¼ ì €ì¥
    clustering_system.save_results(args.output_dir)
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€í•˜ì—¬ ì €ì¥
    clustering_system.save_clustering_results(args.clustering_output)
    
    print("ğŸ‰ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
    print(f"ğŸ’¡ ê³ ì‚¬ì–‘ ì‹œìŠ¤í…œ í™œìš©ìœ¼ë¡œ ìµœì í™”ëœ ì„±ëŠ¥ ì œê³µ")

if __name__ == "__main__":
    main() 