#!/usr/bin/env python3
"""
AI Text Detection Training Script (Trainer-based)
ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì˜ ê°œì„ ëœ ë²„ì „
"""

import os
import sys
import argparse
from datetime import datetime
import shutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback, Qwen3ForSequenceClassification
)
import torch.nn as nn

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from src.utils import (
    load_config, seed_everything, setup_logging,
    get_device, compute_metrics, print_model_info,
    create_directories
)
from src.model import get_model_for_trainer, save_model
from src.dataset import create_dataloaders


def parse_args():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='AI Text Detection Training')
    parser.add_argument(
        '--config',
        type=str,
        default='./config/config.yaml',
        help='Configuration file path'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Output directory (overrides config)'
    )
    parser.add_argument(
        '--resume_from_checkpoint',
        type=str,
        default=None,
        help='Resume training from checkpoint'
    )
    parser.add_argument(
        '--no_cuda',
        action='store_true',
        help='Disable CUDA'
    )
    return parser.parse_args()


def setup_training_args(config: dict, output_dir: str) -> TrainingArguments:
    """í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_dir is None:
        output_dir = os.path.join(
            config['output']['model_save_dir'],
            f"ai_text_detection_{timestamp}"
        )

    return TrainingArguments(
        output_dir=output_dir,

        # í›ˆë ¨ ì„¤ì •
        num_train_epochs=config['training']['num_epochs'],
        per_device_train_batch_size=config['training']['batch_size'],
        per_device_eval_batch_size=config['training']['batch_size'],
        learning_rate=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
        warmup_steps=config['training']['warmup_steps'],

        # í‰ê°€ ì„¤ì •
        eval_strategy=config['evaluation']['strategy'],
        eval_steps=50 if config['evaluation']['strategy'] == "steps" else None,
        save_strategy=config['evaluation']['strategy'],
        save_steps=50 if config['evaluation']['strategy'] == "steps" else None,
        # ìµœì  ëª¨ë¸ ì„ íƒ
        load_best_model_at_end=True,
        metric_for_best_model=config['evaluation']['metric'],
        # ë¡œê¹… ì„¤ì •
        logging_dir=config['output']['logs_dir'],
        logging_steps=10,
        logging_strategy="steps",
        # í•˜ë“œì›¨ì–´ ìµœì í™”
        bf16=True,
        dataloader_num_workers=config['hardware']['dataloader_num_workers'],
        # ê¸°íƒ€ ì„¤ì •
        remove_unused_columns=False,
        report_to=None,  # wandb ë“± ë¹„í™œì„±í™”
        save_total_limit=10,  # ìµœëŒ€ 10ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
        seed=config['data']['random_state'],
        # Gradient clipping
        max_grad_norm=config['training'].get('max_grad_norm', 1.0),
        # ë©”ëª¨ë¦¬ ìµœì í™”
        group_by_length=True,
        dataloader_pin_memory=True,
        gradient_accumulation_steps=2,
    )


def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""

    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    args = parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    print(f"Loaded configuration from: {args.config}")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories(config)

    # ì‹œë“œ ê³ ì •
    seed_everything(config['data']['random_state'])

    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(config['output']['logs_dir'])
    logger.info("Starting AI Text Detection Training")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.no_cuda:
        device = torch.device('cpu')
    else:
        device = get_device(config['hardware']['device'])

    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("Loading model and tokenizer...")
    model, tokenizer = get_model_for_trainer(config)
    model.to(device)

    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print_model_info(model, tokenizer)

    # ë°ì´í„°ë¡œë” ìƒì„±
    logger.info("Creating dataloaders...")
    train_loader, val_loader, test_loader = create_dataloaders(config, tokenizer)

    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
    training_args = setup_training_args(config, args.output_dir)
    logger.info(f"Training arguments configured. Output dir: {training_args.output_dir}")

    # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì»¤ìŠ¤í…€ Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_loader.dataset,
        eval_dataset=val_loader.dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
    )

    # í›ˆë ¨ ì‹œì‘
    logger.info("Starting trainpiping...")
    print("\n" + "=" * 60)
    print("ğŸš€ TRAINING STARTED")
    print("=" * 60)

    train_result = trainer.train()

    # í›ˆë ¨ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… TRAINING COMPLETED")
    print("=" * 60)
    logger.info("Training completed successfully")

    # ìµœì¢… í‰ê°€
    logger.info("Running final evaluation...")
    eval_result = trainer.evaluate()

    print(f"\nğŸ“Š FINAL RESULTS:")
    print(f"   - Best Validation AUC: {eval_result.get('eval_auc', 'N/A'):.4f}")
    print(f"   - Best Validation Accuracy: {eval_result.get('eval_accuracy', 'N/A'):.4f}")
    print(f"   - Training Runtime: {train_result.metrics.get('train_runtime', 'N/A'):.2f} seconds")

    # ëª¨ë¸ ì €ì¥
    logger.info("Saving best model...")
    best_model_path = os.path.join(training_args.output_dir, "best_model")
    trainer.save_model(best_model_path)
    tokenizer.save_pretrained(best_model_path)

    # ì„¤ì • íŒŒì¼ë„ í•¨ê»˜ ì €ì¥
    shutil.copy(args.config, os.path.join(best_model_path, "config.yaml"))

    print(f"âœ… Best model saved to: {best_model_path}")
    logger.info(f"Best model saved to: {best_model_path}")

    print("\nğŸ‰ Training pipeline completed successfully!")


if __name__ == "__main__":
    main() 